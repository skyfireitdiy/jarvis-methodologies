{
  "problem_type": "Token使用信息显示",
  "content": "# Token使用信息显示方法论\n\n## 问题重述\n在LLM应用中，用户需要实时了解模型对话过程中Token的使用情况，包括已用Token数量、剩余Token数量和使用比例，以便更好地管理对话上下文和资源消耗。\n\n## 可复用解决流程\n\n### 1. 确定显示时机\n- **实时显示**：在流式输出过程中，每隔一定时间或内容更新次数显示当前Token使用情况\n- **完成显示**：在对话完成后显示最终的Token统计信息\n\n### 2. 获取Token信息\n需要获取以下关键数据：\n- **已使用Token数**：通过 `get_used_token_count()` 获取历史对话的Token消耗\n- **当前响应Token数**：通过 `get_context_token_count(response)` 计算当前响应的Token数\n- **最大Token限制**：通过 `_get_platform_max_input_token_count()` 获取模型的上下文窗口限制\n\n### 3. 计算使用指标\n```python\ntotal_tokens = history_tokens + current_response_tokens\nmax_tokens = self._get_platform_max_input_token_count()\nusage_percent = (total_tokens / max_tokens) * 100\n```\n\n### 4. 设计可视化展示\n推荐组合多种展示方式：\n- **进度条**：使用字符（如█和░）表示使用比例，颜色根据使用率变化（绿色<80%，黄色<90%，红色>=90%）\n- **百分比**：显示精确的使用百分比\n- **数值统计**：显示具体数值（已用/总数）\n\n### 5. 格式化输出\n使用Rich库进行美化输出：\n```python\nf\"Token: {progress_bar} [{percent_color}]{usage_percent:.1f}% ({total_tokens}/{max_tokens})[/{percent_color}]\"\n```\n\n## 注意事项\n1. **错误处理**：获取Token信息时可能失败，需要添加异常处理，避免阻塞主流程\n2. **性能优化**：在流式输出中限制更新频率（如每10次更新或每3秒），避免界面闪烁\n3. **兼容性**：当无法获取Token信息时，降级显示基础信息（如仅显示耗时）\n4. **平台差异**：不同平台可能有不同的Token计算方式，需要适配具体实现\n\n## 可选步骤\n1. **阈值警告**：当Token使用率超过80%或90%时，显示警告提示\n2. **历史记录**：保存Token使用记录到日志文件，便于后续分析\n3. **预测提示**：根据当前使用率，预测还能进行多少轮对话\n\n## 应用场景\n- 对话界面：在Panel的subtitle中显示实时Token使用情况\n- 完成提示：在对话完成后的总结信息中包含Token统计\n- 简单输出：在命令行模式下使用纯文本显示Token信息",
  "scope": "global"
}